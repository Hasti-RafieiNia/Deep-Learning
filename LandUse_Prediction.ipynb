{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8a22af",
   "metadata": {},
   "source": [
    "*Importing libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e31d8a6-91ad-4d78-adef-9fd89274fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e969230-51c6-4d4a-9616-fdcdf2a4921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from PIL import Image\n",
    "from osgeo import gdal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.interpolate import interpn\n",
    "from skimage.util import view_as_windows\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.layers import Conv2D  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import MaxPooling2D \n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Flatten, Reshape\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.layers import concatenate, Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2cb68-7f5a-450d-879b-6d91cab634c1",
   "metadata": {
    "id": "oN0GABo0WwTZ"
   },
   "source": [
    "-------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d2afa-bd0b-4b96-809f-a385f7c3546c",
   "metadata": {},
   "source": [
    "Preprocess Steps done already in ArcGIS software:\n",
    "\n",
    "Converting the coordinate system of population density rasters from WGS to Canadian Spatial Reference System.\n",
    "\n",
    "Clipping the above rasters based on the case study area and land cover rasters.\n",
    "\n",
    "Equalizing the pixel size of the rasters with each other and resampling them to 30 meter pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e83af3-0303-460f-8923-d1b6910c980d",
   "metadata": {},
   "source": [
    "Using a linear interpolation using meshgrid, so as to create an approximation using 5 existing rasters for the years 2000 to 2020 and save the approximate rasters. With this, all the spatial requirements for the dataset of this project are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523aa95-44c3-4fab-b829-1397a3dfe8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None\n",
    "output = \"D:\\Project\\PopDen\"\n",
    "\n",
    "image_2000 = Image.open(\"D:\\Project\\PopulationDensity\\p2000.tif\")\n",
    "image_2005 = Image.open(\"D:\\Project\\PopulationDensity\\p2005.tif\")\n",
    "image_2010 = Image.open(\"D:\\Project\\PopulationDensity\\p2010.tif\")\n",
    "image_2015 = Image.open(\"D:\\Project\\PopulationDensity\\p2015.tif\")\n",
    "image_2020 = Image.open(\"D:\\Project\\PopulationDensity\\p2020.tif\")\n",
    "\n",
    "arr = np.r_['0,3', image_2000, image_2005, image_2010, image_2015, image_2020]\n",
    "slices, rows, cols = arr.shape\n",
    "\n",
    "#Create meshgrid\n",
    "[X, Y, Z] = np.meshgrid(np.arange(cols), np.arange(rows), np.arange(slices))\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "\n",
    "for year in range(2001, 2020):\n",
    "    \n",
    "    #Determination of weights\n",
    "    [X2, Y2, Z2] = np.meshgrid(np.arange(cols), np.arange(rows), [(year - 2000) / 20 * 4])\n",
    "    Vi = interpn((X, Y, Z), arr, np.array([X2, Y2, Z2]).T)\n",
    "\n",
    "    plt.imshow(Vi[0], cmap=plt.cm.gray)\n",
    "    plt.savefig(os.path.join(output, f\"p{year}.tif\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07653ec6-33f1-49a4-abd5-31fcc744fc70",
   "metadata": {
    "id": "aciiJUNgE5Sh"
   },
   "source": [
    "Creating the required folders for data storage for the years 2000 to 2020.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f96d3a5-49cd-4d50-ad17-da4a95e30573",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\Project\\Data\\Train\"\n",
    "\n",
    "for i in range(2001, 2020):\n",
    "    folder_name = \"y\" + str(i)\n",
    "    new_path = os.path.join(path, folder_name)\n",
    "    os.makedirs(new_path)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "    except FileExistsError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca53003",
   "metadata": {
    "id": "aciiJUNgE5Sh"
   },
   "source": [
    "To extract the patch from the raster images of each year. Since the dimensions of the images are large, we convert each raster into a number of smaller 64 x 64 non-overlapping rasters.\n",
    "\n",
    "In order to prepare the data, negative or nan values will be deleted from our data and finally patches will be saved as two-dimensional arrays in the folder corresponding to each year. Also the data of 2019 is gonna be used as a test of our prediction model, so its arrays are stored  in a separate path.\n",
    "\n",
    "Then, the land cover data will be encoded and converted into 11 bands so that the modeld learns it properly. Then we have to save all the arrays in Float32 format so that the required storage space is not too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b8c9c-077c-4537-af29-3d99c2a6a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pop_dir = \"D:\\\\Project\\\\Data\\\\PopDen\"\n",
    "input_lc_dir = \"D:\\\\Project\\\\Data\\\\LandCover\"\n",
    "output_dir = \"D:\\\\Project\\\\Data\\\\Train\"\n",
    "\n",
    "# Patch size and step for sliding window\n",
    "patch_size = (64, 64) \n",
    "patch_step = 64\n",
    "c = 2001\n",
    "\n",
    "# Loop through each year from 2001 to 2019\n",
    "for year in range(2001, 2020):\n",
    "\n",
    "    # Construct file paths for population density and land cover data\n",
    "    input_pop_file = os.path.join(input_pop_dir, f\"p{year}.tif\") \n",
    "    input_lc_file = os.path.join(input_lc_dir, f\"LC{year}.tif\") \n",
    "    output_subdir = os.path.join(output_dir, f\"y{year}\") \n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_subdir):\n",
    "        os.makedirs(output_subdir)\n",
    "    \n",
    "    # Open raster files\n",
    "    rasterPop_image = gdal.Open(input_pop_file)\n",
    "    rasterLC_image = gdal.Open(input_lc_file)\n",
    "    \n",
    "    # Read raster data as arrays\n",
    "    rasterPop_array = rasterPop_image.ReadAsArray()\n",
    "    rasterLC_array = rasterLC_image.ReadAsArray()\n",
    "\n",
    "    # Handle no-data values in population density array\n",
    "    rasterPop_array[rasterPop_array < 0] = np.nan\n",
    "    col_mean_Pop = np.nanmean(rasterPop_array, axis=0)\n",
    "    index_Pop = np.where(np.isnan(rasterPop_array))\n",
    "    rasterPop_array[index_Pop] = np.take(col_mean_Pop, index_Pop[1])\n",
    "    \n",
    "    # Extract patches using sliding window\n",
    "    pop_patches = view_as_windows(rasterPop_array, patch_size, step=patch_step) \n",
    "    lc_patches = view_as_windows(rasterLC_array, patch_size, step=patch_step) \n",
    "    \n",
    "    num_x = pop_patches.shape[0]\n",
    "    num_y = pop_patches.shape[1] \n",
    "    val = list(np.unique(rasterLC_array))\n",
    "    \n",
    "    # Loop through each patch\n",
    "    for i in range(num_x):\n",
    "        for j in range(num_y):\n",
    "            \n",
    "            # Construct output file path\n",
    "            output_file = os.path.join(output_subdir, f\"arr{year}_{i}_{j}.npz\") \n",
    "            pop_patch = pop_patches[i, j] \n",
    "            lc_patch = lc_patches[i, j] \n",
    "\n",
    "            # One-hot encode land cover data\n",
    "            x, y = lc_patch.shape\n",
    "            lc = np.zeros((x, y, 11), dtype=np.float32)\n",
    "\n",
    "            for k in range(11):\n",
    "                if np.sum(lc_patch == val[k]) > 0:\n",
    "                    X, Y = np.where(lc_patch == val[k])\n",
    "                    lc[X, Y, k] = 1\n",
    "\n",
    "            # Save patches as compressed .npz files\n",
    "            pop_patch = pop_patch.astype(np.float32)\n",
    "            lc = lc.astype(np.float32)\n",
    "            arr = np.concatenate((lc, pop_patch[..., np.newaxis]), axis=2)\n",
    "            arr = arr.astype(np.float32)\n",
    "            np.savez_compressed(output_file, arr=arr)\n",
    "\n",
    "    print(f\"Processed y{c}\")\n",
    "    c += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1faee-f80d-41b7-aea8-c6d80ba20f7a",
   "metadata": {
    "id": "aciiJUNgE5Sh"
   },
   "source": [
    "Creating data in time series format according to CNN-GRU module selected for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349966e-45df-47b1-bf31-95d1899d9e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the data dimensions and time steps\n",
    "num_patches = 2500\n",
    "time_step, width, height, channel = 4, 64, 64, 12\n",
    "num_years = 19\n",
    "u = 0\n",
    "co = 1\n",
    "\n",
    "# Loop through each patch grid (50x50)\n",
    "for i in range(50):\n",
    "    for j in range(50):\n",
    "        c = 1\n",
    "        \n",
    "        # Loop through each year minus the time step\n",
    "        for k in range(num_years - time_step):\n",
    "            # Load the first array for the current year\n",
    "            first_array = np.load(os.path.join(\"D:\\\\Project\\\\Data\\\\Train\", f\"y{2001+k}\", f\"arr{2001+k}_{i}_{j}.npz\"))['arr']\n",
    "            assert first_array.shape == (width, height, channel)\n",
    "            next_arrays = []\n",
    "            \n",
    "            # Load subsequent arrays for the time steps\n",
    "            for z in range(1, time_step + 1):\n",
    "                a = np.load(os.path.join(\"D:\\\\Project\\\\Data\\\\Train\", f\"y{2001+k+z}\", f\"arr{2001+k+z}_{i}_{j}.npz\"))['arr']\n",
    "                next_arrays.append(a)\n",
    "            \n",
    "            # Ensure all arrays have the correct shape\n",
    "            for array in next_arrays:\n",
    "                assert array.shape == (width, height, channel)\n",
    "        \n",
    "            # Stack the arrays to create the input data\n",
    "            input = np.stack([first_array] + next_arrays[:-1], axis=0)\n",
    "            input = input.astype(np.float32)\n",
    "            np.savez_compressed(os.path.join(\"D:\\\\Project\\\\Data\\\\Train\\\\Feature\", f\"input_{u}.npz\"), input=input)\n",
    "            \n",
    "            # Prepare the label data by removing the last channel\n",
    "            label_train = np.delete(next_arrays[-1], -1, axis=2)\n",
    "            label_train = label_train.astype(np.float32)\n",
    "            np.savez_compressed(os.path.join(\"DD:\\\\Project\\\\Data\\\\Train\\\\Label\", f\"label_{u}.npz\"), label=label_train)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Processed sample {c} of patch {co} out of {num_patches}\")\n",
    "            c += 1\n",
    "            u += 1\n",
    "            \n",
    "        co += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7516-aa58-4006-9237-b761aa754a33",
   "metadata": {},
   "source": [
    "Separating the last 2500 arrays related to the year 2019 from the data and saving them in another folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "807ce164-ac03-457c-b018-872fa36c224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"D:\\\\Project\\\\Data\\\\Test\"\n",
    "\n",
    "for folder in [\"Features\", \"label\"]:\n",
    "    folder_path = os.path.join(test_path, folder)\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        \n",
    "        file_number = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "        new_name = f\"{folder}_{file_number - 35000}.npz\"\n",
    "        os.rename(os.path.join(folder_path, file), os.path.join(folder_path, new_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9d8c7-0a2f-4a51-bf8d-3a007ac411d3",
   "metadata": {},
   "source": [
    "Defining the directory path for training/test data and list files in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec7c23cb-8043-40e5-ae0a-c69bb6351868",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_loc_train = \"D:\\Project\\Train\\Features\"\n",
    "input_files_train = os.listdir(input_loc_train)\n",
    "\n",
    "label_loc_train = \"D:\\Project\\Train\\Label\"\n",
    "label_files_train = os.listdir(label_loc_train)\n",
    "\n",
    "input_loc_test = \"D:\\Test\\Features\"\n",
    "input_files_test = os.listdir(input_loc_test)\n",
    "\n",
    "label_loc_test = \"D:\\Project\\Test\\Label\"\n",
    "label_files_test = os.listdir(label_loc_test)\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for training/test data\n",
    "image_label_map = {\"input_{}.npz\".format(i): \"label_{}.npz\".format(i)\n",
    "                    for i in range(len(input_files_train))}\n",
    "\n",
    "image_label_map_val = {\"input_{}.npz\".format(i): \"label_{}.npz\".format(i)\n",
    "                       for i in range(len(input_files_test))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9890aec-d3d6-427a-a591-da1a8200e080",
   "metadata": {},
   "source": [
    "Creating two Data Generator classes to avoid memory errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2c61c09-c34c-4224-86c4-2cc571fd2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step, width, height, channel = 4, 64, 64, 12\n",
    "\n",
    "# Define a custom data generator class for training/test data\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size = 4, dim = (time_step, width, height, channel), shuffle = True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2], self.dim[3]], dtype = np.float32)\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 11], dtype = np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(input_loc_train, i)\n",
    "            y_file_path = os.path.join(label_loc_train, image_label_map.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :, :] = np.load(x_file_path)['input']\n",
    "            \n",
    "            # Load labels    \n",
    "            y[c, :, :, :] = np.load(y_file_path)['label']\n",
    "            \n",
    "\n",
    "            c += 1\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ba14b-538a-469d-a3a9-cdd742e14c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size = 4, dim = (time_step, width, height, channel), shuffle = True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2], self.dim[3]], dtype = np.float32)\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 11], dtype = np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(input_loc_test, i)\n",
    "            y_file_path = os.path.join(label_loc_test, image_label_map_val.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :, :] = np.load(x_file_path)['input_train']\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)['label_train']\n",
    "\n",
    "            c += 1\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d0c37c0-4a20-4feb-bf0f-e63496950e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom data generators for training and validation data\n",
    "training_generator = DataGenerator(input_files_train)\n",
    "validation_generator = ValDataGenerator(input_files_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14e48b-3196-451f-b433-0610f5709adf",
   "metadata": {},
   "source": [
    "Since the data used in this project is a time series, time distributed convolutional layers are used so that the model has the ability to understand time and the extracted features are also in terms of time. We also added a regularization kernel to adding penalty factors to the network layers to alter the weight propagation through the layers which facilitate the model to converge optimally. In addition, max pooling and CNN layers with different dilation_rate are used to extract features with different and better dimensions and to reduce the amount of data.\n",
    "\n",
    "After CNN layers and their normalization by Batch Normalization are done, it is time for the GRU layers to form the hybrid architecture we need. These layers, like RNN layers, can model the time step. Finally, by adding a few Dense and dropout layers to avoid overfitting the model, the computational part of the model is completed and  output layer with 11 neurons to recognize the 11 classes in the land cover rasters in binary form are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd4e9021-bf97-445f-97f3-33aebc616537",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer=keras.regularizers.l2(1e-2)\n",
    "ALPHA = 0.8\n",
    "BETA = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352b5802-edad-4f93-93e1-23b94a033914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_GRU_Model(input_shape):\n",
    "    \"\"\"\n",
    "    Creates a CNN-GRU model for spatiotemporal data processing.\n",
    "\n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data (time_steps, width, height, channels).\n",
    "\n",
    "    Returns:\n",
    "    model (tf.keras.Model): Compiled CNN-GRU model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer with the given shape\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # First convolutional block with TimeDistributed layers\n",
    "    x = TimeDistributed(Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizer))(inputs) \n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x) \n",
    "    \n",
    "    # Second convolutional block\n",
    "    x = TimeDistributed(Conv2D(128, (3,3), activation='relu', kernel_regularizer=regularizer))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x) \n",
    "\n",
    "    # Atrous (dilated) convolutions for multi-scale context aggregation\n",
    "    x2 = TimeDistributed(Conv2D(64, (3,3), dilation_rate=3, activation='relu', padding='same', kernel_regularizer=regularizer))(x)\n",
    "    x3 = TimeDistributed(Conv2D(64, (3,3), dilation_rate=6, activation='relu', padding='same', kernel_regularizer=regularizer))(x)    \n",
    "    x4 = TimeDistributed(Conv2D(64, (3,3), dilation_rate=12, activation='relu', padding='same', kernel_regularizer=regularizer))(x)\n",
    "    x5 = TimeDistributed(Conv2D(64, (3,3), dilation_rate=18, activation='relu', padding='same', kernel_regularizer=regularizer))(x)\n",
    "    x6 = TimeDistributed(Conv2D(64, (3,3), dilation_rate=24, activation='relu', padding='same', kernel_regularizer=regularizer))(x)  \n",
    "    \n",
    "    # Concatenate the outputs of the atrous convolutions\n",
    "    x6 = concatenate([x6, x5, x2, x3, x4])\n",
    "    \n",
    "    # Additional convolutional layers\n",
    "    x7 = TimeDistributed(Conv2D(32, (1,1), padding='same', kernel_regularizer=regularizer))(x6) \n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x7)\n",
    "    x = TimeDistributed(Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizer))(x) \n",
    "    x = TimeDistributed(Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizer))(x) \n",
    "    x = TimeDistributed(BatchNormalization())(x) \n",
    "    x = TimeDistributed(Flatten())(x) \n",
    "    \n",
    "    # GRU layers for temporal processing\n",
    "    x = GRU(64, return_sequences=True)(x) \n",
    "    x = GRU(64)(x) \n",
    "\n",
    "    # Fully connected layers with Dropout\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=regularizer)(x)\n",
    "    x = Dropout(rate=0.3)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=regularizer)(x) \n",
    "    x = Dropout(rate=0.3)(x)\n",
    "    \n",
    "    # Output layer with sigmoid activation and reshaping to the desired output shape\n",
    "    outputs = Dense(64*64*11, activation='sigmoid')(x) \n",
    "    outputs = Reshape((64, 64, 11))(outputs) \n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda0117-b8c5-4cf9-952e-e33be2ca7b67",
   "metadata": {},
   "source": [
    "Creating two functions to calculate the confusion matrix, TverskyLoss, jac_distance and F1 score to use them for loss functios and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51fde3aa-4700-4378-87dc-bb95ddc7431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Tversky loss for image segmentation tasks.\n",
    "\n",
    "    The Tversky loss is a generalization of the Dice coefficient, which allows for \n",
    "    asymmetric penalties for false positives and false negatives. This can be useful \n",
    "    in medical image segmentation where the cost of false positives and false negatives \n",
    "    may differ.\n",
    "\n",
    "    Parameters:\n",
    "    targets (tensor): Ground truth binary masks.\n",
    "    inputs (tensor): Predicted binary masks.\n",
    "    alpha (float): Weight of false positives.\n",
    "    beta (float): Weight of false negatives.\n",
    "    smooth (float): Smoothing factor to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "    float: Tversky loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the input and target tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    TP = K.sum((inputs * targets))\n",
    "    FP = K.sum(((1 - targets) * inputs))\n",
    "    FN = K.sum((targets * (1 - inputs)))\n",
    "    \n",
    "    # Compute the Tversky index\n",
    "    Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n",
    "\n",
    "    # Return the Tversky loss\n",
    "    return (1 - Tversky)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e04662f4-9df3-46eb-9567-cef7181263e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes the Intersection over Union (IoU) metric.\n",
    "\n",
    "    The IoU metric, also known as the Jaccard index, is used to evaluate the \n",
    "    similarity between two sets, typically used for image segmentation tasks.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): Ground truth binary masks.\n",
    "    y_pred (tensor): Predicted binary masks.\n",
    "    smooth (float): Smoothing factor to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "    float: IoU value.\n",
    "    \"\"\"\n",
    "    # Calculate the intersection and union of the true and predicted masks\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum_ = K.sum(y_true + y_pred)\n",
    "    \n",
    "    # Compute the IoU\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    \n",
    "    return jac\n",
    "\n",
    "def jac_distance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard distance, which is the complement of the IoU.\n",
    "\n",
    "    The Jaccard distance is used as a loss function for training segmentation models.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): Ground truth binary masks.\n",
    "    y_pred (tensor): Predicted binary masks.\n",
    "\n",
    "    Returns:\n",
    "    float: Jaccard distance value.\n",
    "    \"\"\"\n",
    "    # Flatten the input tensors\n",
    "    y_truef = K.flatten(y_true)\n",
    "    y_predf = K.flatten(y_pred)\n",
    "    \n",
    "    # Return the negative IoU as the Jaccard distance\n",
    "    return -iou(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "676dec3e-b149-4e82-afc9-546182a3bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(label, predict):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for binary classification tasks.\n",
    "\n",
    "    The confusion matrix is a table that is often used to describe the performance \n",
    "    of a classification model. It contains the counts of true positives, true negatives, \n",
    "    false positives, and false negatives.\n",
    "\n",
    "    Parameters:\n",
    "    label (tensor): Ground truth binary labels.\n",
    "    predict (tensor): Predicted binary labels.\n",
    "\n",
    "    Returns:\n",
    "    np.array: 2x2 confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape the labels and predictions to 1D tensors\n",
    "    label = tf.reshape(label, [-1])\n",
    "    predict = tf.reshape(predict, [-1])\n",
    "\n",
    "    # Cast the labels and predictions to boolean\n",
    "    label = tf.cast(label, tf.bool)\n",
    "    predict = tf.cast(predict, tf.bool)\n",
    "    \n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    true_positive = tf.reduce_sum(tf.cast(tf.logical_and(label, predict), tf.int32))\n",
    "    true_negative = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(label), tf.logical_not(predict)), tf.int32))\n",
    "    false_positive = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(label), predict), tf.int32))\n",
    "    false_negative = tf.reduce_sum(tf.cast(tf.logical_and(label, tf.logical_not(predict)), tf.int32))\n",
    "    \n",
    "    # Return the confusion matrix as a numpy array\n",
    "    return np.array([[true_positive, false_positive], [false_negative, true_negative]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcddab81-1137-4b92-ba67-be1c4d2dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the F1 score for binary classification tasks.\n",
    "\n",
    "    The F1 score is the harmonic mean of precision and recall, providing a balance \n",
    "    between the two. It is especially useful for imbalanced datasets.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): Ground truth binary labels.\n",
    "    y_pred (tensor): Predicted binary labels.\n",
    "\n",
    "    Returns:\n",
    "    float: F1 score value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = Precision()(y_true, y_pred)\n",
    "    recall = Recall()(y_true, y_pred)\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "    return f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c90a32-a924-410b-8096-4c9dd4cb9203",
   "metadata": {},
   "source": [
    "Creating an instance of the custom neural network model with the specified input shape.Then compiling the model using the precision and recall criteria and the binary_crossentropy loss function.Also adding 2 callbacks to save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa793695-11cf-4ab3-ac5d-94405fc71e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 64, 64, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 4, 62, 62, 12 13952       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 4, 31, 31, 12 0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 4, 29, 29, 25 295168      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 4, 14, 14, 25 0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 4, 14, 14, 64 147520      time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 4, 14, 14, 64 147520      time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 4, 14, 14, 64 147520      time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 4, 14, 14, 64 147520      time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 4, 14, 14, 64 147520      time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4, 14, 14, 32 0           time_distributed_8[0][0]         \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "                                                                 time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 4, 14, 14, 32 10272       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 4, 7, 7, 32)  0           time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 4, 5, 5, 32)  9248        time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 4, 3, 3, 32)  9248        time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 4, 3, 3, 32)  128         time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 4, 288)       0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 4, 64)        67968       time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 64)           24960       gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          8320        gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 45056)        1486848     dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 64, 64, 11)   0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,674,048\n",
      "Trainable params: 2,673,984\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model with the given input shape\n",
    "model = CNN_GRU_Model((time_step, width, height, channel))\n",
    "\n",
    "# Define the Hyperparameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)\n",
    "model.compile(loss = TverskyLoss, optimizer = optimizer, metrics = [Recall(), Precision(), jac_distance])\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 25)\n",
    "\n",
    "callbacks = [ModelCheckpoint(\"D:/Project/Best.h5\",\n",
    "                             verbose = 1, save_best_only = True), early_stopping]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506763f-ef22-4dd1-bc95-03984e65b6e0",
   "metadata": {},
   "source": [
    "Fitthing the created model on our training data with 20 epochs and after finishing the training process of the model, we save the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "52d15270-27c1-473e-9b72-3f91faf3e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8750/8750 [==============================] - 829s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2467 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3059\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.54849\n",
      "Epoch 2/20\n",
      "8750/8750 [==============================] - 810s 93ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2463 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3052\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54849 to 0.54848, saving model to D:/Learning/GeoAi/Week8/Project\\BestModel.h5\n",
      "Epoch 3/20\n",
      "8750/8750 [==============================] - 797s 91ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2465 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3071\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54848\n",
      "Epoch 4/20\n",
      "8750/8750 [==============================] - 825s 94ms/step - loss: 0.6222 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2464 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3052\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54848\n",
      "Epoch 5/20\n",
      "8750/8750 [==============================] - 835s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2464 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3064\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54848 to 0.54846, saving model to D:/Learning/GeoAi/Week8/Project\\BestModel.h5\n",
      "Epoch 6/20\n",
      "8750/8750 [==============================] - 833s 95ms/step - loss: 0.6222 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2467 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3060\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54846\n",
      "Epoch 7/20\n",
      "8750/8750 [==============================] - 831s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2467 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3052\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.54846\n",
      "Epoch 8/20\n",
      "8750/8750 [==============================] - 830s 95ms/step - loss: 0.6222 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2465 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3062\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.54846\n",
      "Epoch 9/20\n",
      "8750/8750 [==============================] - 830s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2463 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3058\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.54846\n",
      "Epoch 10/20\n",
      "8750/8750 [==============================] - 822s 94ms/step - loss: 0.6222 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2466 - val_loss: 0.5486 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3061\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.54846\n",
      "Epoch 11/20\n",
      "8750/8750 [==============================] - 830s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2466 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3058\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.54846\n",
      "Epoch 12/20\n",
      "8750/8750 [==============================] - 832s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2466 - val_loss: 0.5486 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3055\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.54846\n",
      "Epoch 13/20\n",
      "8750/8750 [==============================] - 831s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2466 - val_loss: 0.5486 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3059\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.54846\n",
      "Epoch 14/20\n",
      "8750/8750 [==============================] - 831s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2467 - val_loss: 0.5484 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3068\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.54846 to 0.54843, saving model to D:/Learning/GeoAi/Week8/Project\\BestModel.h5\n",
      "Epoch 15/20\n",
      "8750/8750 [==============================] - 830s 95ms/step - loss: 0.6222 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2467 - val_loss: 0.5483 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3059\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.54843 to 0.54828, saving model to D:/Learning/GeoAi/Week8/Project\\BestModel.h5\n",
      "Epoch 16/20\n",
      "8750/8750 [==============================] - 831s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2465 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3060\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.54828\n",
      "Epoch 17/20\n",
      "8750/8750 [==============================] - 831s 95ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2465 - val_loss: 0.5486 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3063\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.54828\n",
      "Epoch 18/20\n",
      "8750/8750 [==============================] - 829s 95ms/step - loss: 0.6222 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2464 - val_loss: 0.5485 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3052\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.54828\n",
      "Epoch 19/20\n",
      "8750/8750 [==============================] - 827s 94ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2467 - val_loss: 0.5484 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3061\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.54828\n",
      "Epoch 20/20\n",
      "8750/8750 [==============================] - 827s 94ms/step - loss: 0.6221 - recall_6: 0.3886 - precision_6: 0.3758 - jac_distance: -0.2465 - val_loss: 0.5484 - val_recall_6: 0.4614 - val_precision_6: 0.4497 - val_jac_distance: -0.3064\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.54828\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_generator, epochs = 20, validation_data = validation_generator, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0e4d0-2927-4a5f-aba2-c0e957e92036",
   "metadata": {},
   "source": [
    "Loading the best model to predict the test data and evaluate it based on the mentioned criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ddfde96-db5a-4a52-ae2c-0e3354d78a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel = load_model(\"D:\\Project\\Best.h5\", custom_objects = {'TverskyLoss':TverskyLoss, 'jac_distance':jac_distance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "258d7bba-d257-4b60-bec4-9226c51d5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = MyModel.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "339b022c-dc80-4c1a-b6e6-7838cfad8e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 11s 15ms/step - loss: 0.5485 - recall_6: 0.4614 - precision_6: 0.4497 - jac_distance: -0.3056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5485276579856873,\n",
       " 0.4614284634590149,\n",
       " 0.44965338706970215,\n",
       " -0.3055548071861267]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32563565-9d52-41cb-9a0a-bc5927f06e91",
   "metadata": {},
   "source": [
    "Creating a threshold on the output data so that each element becomes 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2bbbd54-6f2c-4f2f-b7a3-b345085be786",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = label_pred > 0.5\n",
    "label_pred = np.where(condition, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027edbc5-87e2-4c73-bb60-f96de3f5c29f",
   "metadata": {},
   "source": [
    "Creating a matrix to cover all the project data using the stored arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6c1612f-69ee-45c7-96a7-e1fa9a17f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store input and label arrays\n",
    "input_list = []\n",
    "label_list = []\n",
    "\n",
    "# Get the number of files in the input directory\n",
    "num_files = len(os.listdir(\"D:\\\\Project\\\\Test\\\\Features\"))\n",
    "\n",
    "# Loop through each file index\n",
    "for i in range(num_files):\n",
    "   \n",
    "    input_file = \"D:\\\\Project\\\\Test\\\\Features\\\\feature\" + str(i) + '.npz'\n",
    "    label_file = \"D:\\\\Project\\\\Test\\\\Features\\\\Label\\\\label_\" + str(i) + '.npz'\n",
    "    \n",
    "    # Load the input and label arrays from the .npz files\n",
    "    input_array = np.load(input_file)['feature_train']\n",
    "    label_array = np.load(label_file)['label_train']\n",
    "    \n",
    "    # Append the arrays to the respective lists\n",
    "    input_list.append(input_array)\n",
    "    label_list.append(label_array)\n",
    "\n",
    "# Stack the lists into 5D and 4D arrays respectively\n",
    "input_5D = np.stack(input_list, axis=0)\n",
    "label_4D = np.stack(label_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590b5f1-88bd-422e-b5ab-5c5ea5cfd321",
   "metadata": {},
   "source": [
    "Evaluationing through confusion matrix and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6c0ffc5-9ceb-403e-8744-37d9bcdda160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>4614568.0</td>\n",
       "      <td>5647932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5386046.0</td>\n",
       "      <td>96991454.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Positive    Negative\n",
       "True   4614568.0   5647932.0\n",
       "False  5386046.0  96991454.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_confusion_matrix = np.zeros((2, 2))\n",
    "\n",
    "for i in range(2500):\n",
    "    total_confusion_matrix += confusion_matrix(label_4D[i], label_pred[i])\n",
    "\n",
    "total_confusion_matrix = pd.DataFrame(total_confusion_matrix, index=['True', 'False'], columns=['Positive', 'Negative'])\n",
    "total_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f084d69-fa24-4011-85b7-c3037aae6056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the dataframe as a heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sb.heatmap(total_confusion_matrix, annot=True, fmt='.0f', cmap=\"BuPu\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554afeee-95e4-4718-be49-afb13bda0e7e",
   "metadata": {
    "id": "zAwRP94YJsrb"
   },
   "source": [
    "-------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89eec91-c5fb-44f7-a227-a2386c5fc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "\n",
    "for i in range(2500):\n",
    "    label = label_4D[i].flatten()\n",
    "    predict = label_pred[i].flatten()\n",
    "    f1 = f1_score(label, predict)\n",
    "    f1_scores.append(f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "276f562e-0786-4064-9700-183a07ebd24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.4502526237820023\n"
     ]
    }
   ],
   "source": [
    "mean_f1_score = (sum(f1_scores)/len(f1_scores))\n",
    "print('F1 Score : ', mean_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
